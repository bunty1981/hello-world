{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2f7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'punkt_tab' is already installed. Skipping download.\n",
      "'stopwords' not found. Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/BHAGARO1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/BHAGARO1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 'stopwords'.\n",
      "'wordnet' not found. Downloading...\n",
      "Successfully downloaded 'wordnet'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure Jupyter automatically reloads any module that is modified \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Import custom modules\n",
    "import my_string_preprocessing as msp\n",
    "import importlib\n",
    "\n",
    "# # IF USING MANUAL RELOAD (if autoreload does not work) After making changes to my_sting_preprocessing.py in your editor, run this cell:\n",
    "# importlib.reload(msp)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68dbda20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unnamed: 0   1000 non-null   int64 \n",
      " 1   index        1000 non-null   int64 \n",
      " 2   title        1000 non-null   object\n",
      " 3   pubDate      1000 non-null   object\n",
      " 4   guid         1000 non-null   object\n",
      " 5   link         1000 non-null   object\n",
      " 6   description  1000 non-null   object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bbc_data = pd.read_csv('bbc_news.csv')\n",
    "bbc_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdafa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m6/zf9l8nt11dlg_sq_4flxqr8c0000gp/T/ipykernel_49933/3241840223.py:4: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  titles['raw_tokens'] = titles['title'].apply(msp.preproc_text_string, lemmatize_flag)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7649\n",
      "7649\n",
      "7649\n",
      "The lemmatization traversal methods are exact replicas:  True\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "titles = pd.DataFrame(bbc_data['title'])\n",
    "lemmatize_flag = False\n",
    "titles['raw_tokens'] = titles['title'].apply(msp.preproc_text_string, lemmatize_flag)\n",
    "# Alternative method using .apply for lemmatization\n",
    "titles['lemmatized_tokens'] = titles.apply(lambda xx: ' '.join([lemmatizer.lemmatize(token) for token in word_tokenize(xx['raw_tokens'])]), axis=1)\n",
    "titles['lemmatized_tokens'] = titles.apply(lambda xx: word_tokenize(xx['lemmatized_tokens']), axis=1) # Alternative method \n",
    "lemmatized_tokens_list_alt = sum(titles['lemmatized_tokens'],[])\n",
    "\n",
    "# Generate flat list of raw_tokens & lemmatized tokens\n",
    "titles['raw_tokens'] = titles.apply(lambda xx: word_tokenize(xx['raw_tokens']), axis=1)\n",
    "raw_tokens_list = sum(titles['raw_tokens'],[])\n",
    "lemmatized_tokens_list = [lemmatizer.lemmatize(token) for token in raw_tokens_list] # Mainline (Simpler) method\n",
    "\n",
    "print(len(raw_tokens_list) )\n",
    "print(len(lemmatized_tokens_list))\n",
    "print(len(lemmatized_tokens_list_alt))\n",
    "\n",
    "print(\"The lemmatization traversal methods are exact replicas: \", lemmatized_tokens_list_alt == lemmatized_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd243946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the original tokens and the cleaned tokens\n",
    "raw_tokens_list = sum(titles['title'].tolist(), [])\n",
    "clean_tokens_list = sum(titles['clean_tokens'].tolist(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f57e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagging using nltk\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# create a spacy_document\n",
    "spacy_docs = [nlp(doc) for doc in titles['clean_tokens']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
